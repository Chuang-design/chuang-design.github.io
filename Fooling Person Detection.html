<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Adversarial Patch Against YOLO | Chuang Chen</title>
  <link rel="stylesheet" href="style.css" />
  <link rel="icon" href="images/favicon.ico" type="image/x-icon" />
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .content-wrapper img {
      display: block;
      margin: 2rem auto;
      max-width: 100%;
      height: auto;
    }
    .caption {
      text-align: center;
      font-size: 0.9rem;
      color: #555;
      margin-top: -1rem;
    }
  </style>
</head>
<body>
  <header>
    <div class="header-inner">
      <div class="site-title"><a href="index.html">Chuang Chen</a></div>
      <button class="menu-toggle" aria-label="Toggle navigation">☰</button>
      <nav class="main-nav">
        <a href="project.html">Project</a>
        <a href="publication.html">Publication</a>
        <a href="cv.html">CV</a>
        <a href="about.html">About</a>
      </nav>
    </div>
  </header>

  <main class="content-wrapper">
    <h1 class="page-heading">Adversarial Patch Attacks Against Person Detection</h1>

    <p>
        This project explores adversarial patch attacks on object detection systems. By generating and applying carefully optimized visual patterns, we aim to mislead YOLOv2 into failing to detect human figures. The approach builds on the idea that small, targeted modifications to an image can significantly alter neural network outputs. We generate printable patches that remain effective under physical-world conditions, using a loss function combining printability, visual smoothness, and detection confidence.
    </p>

    <p>
        Our work enhances existing methods in two ways. First, by improving stealth—our patches can be nearly black yet still succeed in attacks.
    </p>
    <img src="images/attack/attack_4.gif" alt="attack_1" />
    <p>
        Ssecond, by enabling misclassification, causing the model to mistake people for other objects such as bears or umbrellas.
    </p>
    <img src="images/attack/attack_5.gif" alt="attact_2" />
    <p class="caption">Misclassified as a bear</p>
    
    <img src="images/attack/attack_6.gif" alt="attack_3" />
    <p class="caption">Misclassified as an umbrella</p>
  </main>

  <footer>
    &copy; 2025 Chuang Chen · <a href="mailto:chenchuang@zju.edu.cn">chenchuang@zju.edu.cn</a>
  </footer>

  <script>
    document.addEventListener("DOMContentLoaded", () => {
      const toggle = document.querySelector(".menu-toggle");
      const nav = document.querySelector(".main-nav");
      toggle.addEventListener("click", () => {
        nav.classList.toggle("show");
      });
    });
  </script>
</body>
</html>
